{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算 Transformers 模型參數量與量化等級"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安裝依賴套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Package            Version\n",
      "------------------ -----------\n",
      "asttokens          3.0.0\n",
      "certifi            2025.1.31\n",
      "charset-normalizer 3.4.1\n",
      "comm               0.2.2\n",
      "debugpy            1.8.12\n",
      "decorator          5.1.1\n",
      "exceptiongroup     1.2.2\n",
      "executing          2.2.0\n",
      "filelock           3.17.0\n",
      "fsspec             2025.2.0\n",
      "huggingface-hub    0.29.0\n",
      "idna               3.10\n",
      "ipykernel          6.29.5\n",
      "ipython            8.32.0\n",
      "jedi               0.19.2\n",
      "jupyter_client     8.6.3\n",
      "jupyter_core       5.7.2\n",
      "matplotlib-inline  0.1.7\n",
      "nest-asyncio       1.6.0\n",
      "numpy              2.2.3\n",
      "packaging          24.2\n",
      "parso              0.8.4\n",
      "pexpect            4.9.0\n",
      "pip                25.0.1\n",
      "pip-autoremove     0.10.0\n",
      "platformdirs       4.3.6\n",
      "prompt_toolkit     3.0.50\n",
      "psutil             7.0.0\n",
      "ptyprocess         0.7.0\n",
      "pure_eval          0.2.3\n",
      "Pygments           2.19.1\n",
      "python-dateutil    2.9.0.post0\n",
      "python-dotenv      1.0.1\n",
      "PyYAML             6.0.2\n",
      "pyzmq              26.2.1\n",
      "regex              2024.11.6\n",
      "requests           2.32.3\n",
      "safetensors        0.5.2\n",
      "setuptools         75.8.0\n",
      "six                1.17.0\n",
      "stack-data         0.6.3\n",
      "tokenizers         0.21.0\n",
      "tornado            6.4.2\n",
      "tqdm               4.67.1\n",
      "traitlets          5.14.3\n",
      "transformers       4.49.0\n",
      "typing_extensions  4.12.2\n",
      "urllib3            2.3.0\n",
      "wcwidth            0.2.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U pip setuptools pip-autoremove\n",
    "%pip install -q -U transformers\n",
    "%pip install -q -U python-dotenv\n",
    "%pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根據 Transformers 模型各層加總計算模型參數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, PretrainedConfig\n",
    "\n",
    "def load_model_config(model_id: str, **kwargs) -> PretrainedConfig:\n",
    "    config = AutoConfig.from_pretrained(model_id, **kwargs)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelLayerParams:\n",
    "    \n",
    "    vocab_size: Optional[int]\n",
    "    \n",
    "    hidden_size: Optional[int]\n",
    "    \n",
    "    intermediate_size: Optional[int] \n",
    "    \n",
    "    num_attention_heads: Optional[int]\n",
    "    \n",
    "    head_dim: Optional[int]\n",
    "    \n",
    "    num_key_value_heads: Optional[int]\n",
    "    \n",
    "    num_hidden_layers: Optional[int]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "BILLION = 10 ** 9\n",
    "\n",
    "def calc_model_params(model_config: PretrainedConfig) -> float:\n",
    "    \n",
    "    def calc_embedding_layer_params(hidden_size: int, vocab_size: int):\n",
    "        return hidden_size * vocab_size    \n",
    "    \n",
    "    def calc_multi_head_layer_params(\n",
    "        hidden_size: int, \n",
    "        num_key_value_heads: int, \n",
    "        head_dim: int\n",
    "    ) -> int:\n",
    "        q_layer_params = hidden_size ** 2\n",
    "        k_layer_params = hidden_size * num_key_value_heads * head_dim\n",
    "        v_layer_params = k_layer_params\n",
    "        o_layer_params = hidden_size ** 2\n",
    "        \n",
    "        return q_layer_params + k_layer_params + v_layer_params + o_layer_params    \n",
    "    \n",
    "    def calc_ffn_layer_params(hidden_size: int, intermediate_size: int) -> int:\n",
    "        ffn_first_layer = hidden_size * 2 * intermediate_size\n",
    "        ffn_second_layer = intermediate_size * hidden_size\n",
    "        \n",
    "        return ffn_first_layer + ffn_second_layer\n",
    "    \n",
    "    def calc_norm_layer_params(hidden_size: int) -> int:\n",
    "        return 2 * hidden_size\n",
    "    \n",
    "    def calc_transformer_layer_params(\n",
    "        num_hidden_layers: int, \n",
    "        multi_head_layer_params: int, \n",
    "        ffn_layer_params: int,\n",
    "        norm_layer_params: int\n",
    "    ) -> int:\n",
    "        return num_hidden_layers * (multi_head_layer_params + ffn_layer_params + norm_layer_params)\n",
    "    \n",
    "    def validate_model_layer_params(params: ModelLayerParams) -> bool:\n",
    "        return all([v is not None for v in params.__dict__.values()])\n",
    "    \n",
    "    total_model_params: float = 0\n",
    "    \n",
    "    vocab_size: Optional[int] = None\n",
    "    hidden_size: Optional[int] = None\n",
    "    intermediate_size: Optional[int] = None\n",
    "    num_attention_heads: Optional[int] = None\n",
    "    head_dim: Optional[int] = None\n",
    "    num_key_value_heads: Optional[int] = None\n",
    "    num_hidden_layers: Optional[int] = None\n",
    "\n",
    "    if hasattr(model_config, 'vocab_size'):\n",
    "        vocab_size = model_config.vocab_size\n",
    "    elif hasattr(model_config, 'text_config'):\n",
    "        vocab_size = model_config.text_config.vocab_size\n",
    "    \n",
    "    if hasattr(model_config, 'hidden_size'):\n",
    "        hidden_size = model_config.hidden_size\n",
    "    elif hasattr(model_config, 'text_config'):\n",
    "        hidden_size = model_config.text_config.hidden_size\n",
    "        \n",
    "    if hasattr(model_config, 'intermediate_size'):\n",
    "        intermediate_size = model_config.intermediate_size\n",
    "    elif hasattr(model_config, 'text_config'):\n",
    "        intermediate_size = model_config.text_config.intermediate_size\n",
    "        \n",
    "    if hasattr(model_config, 'num_attention_heads'):\n",
    "        num_attention_heads = model_config.num_attention_heads\n",
    "    elif hasattr(model_config, 'text_config'):\n",
    "        num_attention_heads = model_config.text_config.num_attention_heads\n",
    "        \n",
    "    if hasattr(model_config, 'head_dim'):\n",
    "        if model_config.head_dim is not None:\n",
    "            head_dim = model_config.head_dim\n",
    "    elif hasattr(model_config, 'text_config'):\n",
    "        if hasattr(model_config.text_config, 'head_dim'):\n",
    "            head_dim = model_config.text_config.head_dim\n",
    "    else:\n",
    "        head_dim = hidden_size // num_attention_heads\n",
    "        \n",
    "    if hasattr(model_config, 'num_key_value_heads'):\n",
    "        num_key_value_heads = model_config.num_key_value_heads\n",
    "    elif hasattr(model_config, 'text_config'):\n",
    "        num_key_value_heads = model_config.text_config.num_key_value_heads\n",
    "        \n",
    "    if hasattr(model_config, 'num_hidden_layers'):\n",
    "        num_hidden_layers = model_config.num_hidden_layers\n",
    "    elif hasattr(model_config, 'text_config'):\n",
    "        num_hidden_layers = model_config.text_config.num_hidden_layers\n",
    "    \n",
    "    model_layer_params = ModelLayerParams(\n",
    "        vocab_size=vocab_size,\n",
    "        hidden_size=hidden_size,\n",
    "        intermediate_size=intermediate_size,\n",
    "        num_attention_heads=num_attention_heads,\n",
    "        head_dim=head_dim,\n",
    "        num_key_value_heads=num_key_value_heads,\n",
    "        num_hidden_layers=num_hidden_layers\n",
    "    )\n",
    "    print(model_layer_params)\n",
    "    \n",
    "    if not validate_model_layer_params(model_layer_params):\n",
    "        return 0.0\n",
    "    \n",
    "    embedding_layer_params = calc_embedding_layer_params(hidden_size, vocab_size)\n",
    "    multi_head_layer_params = calc_multi_head_layer_params(hidden_size, num_key_value_heads, head_dim)\n",
    "    ffn_layer_params = calc_ffn_layer_params(hidden_size, intermediate_size)\n",
    "    norm_layer_params = calc_norm_layer_params(hidden_size)\n",
    "    transformer_layer_params = calc_transformer_layer_params(num_hidden_layers, multi_head_layer_params, ffn_layer_params, norm_layer_params)\n",
    "    \n",
    "    total_model_params += embedding_layer_params\n",
    "    total_model_params += transformer_layer_params\n",
    "    \n",
    "    return round(total_model_params / BILLION, 2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根據 Transformers 模型的 Config，計算量化等級"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "from transformers import AwqConfig, BitsAndBytesConfig, GPTQConfig\n",
    "\n",
    "def calc_model_quantization_level(model_config: PretrainedConfig) -> int:\n",
    "    if hasattr(model_config, 'quantization_config') and model_config.quantization_config:\n",
    "        quantization_config = model_config.quantization_config\n",
    "        \n",
    "        if isinstance(quantization_config, dict):\n",
    "            if quantization_config.get(\"quant_method\") == \"bitsandbytes\":\n",
    "                if quantization_config.get(\"load_in_4bit\") and bool(quantization_config[\"load_in_4bit\"]): \n",
    "                    return 4\n",
    "                if quantization_config.get(\"load_in_8bit\") and bool(quantization_config[\"load_in_8bit\"]): \n",
    "                    return 8\n",
    "                \n",
    "            if quantization_config.get(\"quant_method\") == \"fp8\":\n",
    "                return 8\n",
    "                \n",
    "            if quantization_config.get(\"bits\"):\n",
    "                return int(quantization_config[\"bits\"])\n",
    "            \n",
    "        if isinstance(quantization_config, Union[AwqConfig, GPTQConfig]):\n",
    "            return quantization_config.bits\n",
    "        \n",
    "        if isinstance(quantization_config, BitsAndBytesConfig):\n",
    "            if quantization_config.load_in_4bit:\n",
    "                return 4\n",
    "            if quantization_config.load_in_8bit:\n",
    "                return 8\n",
    "    else:\n",
    "        return 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelParams:\n",
    "    \n",
    "    model_id : str\n",
    "    \n",
    "    model_config: PretrainedConfig\n",
    "    \n",
    "    params: float\n",
    "    \n",
    "    quantization_level: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelLayerParams(vocab_size=128256, hidden_size=4096, intermediate_size=14336, num_attention_heads=32, head_dim=128, num_key_value_heads=8, num_hidden_layers=32)\n",
      "Model: meta-llama/Llama-3.1-8B-Instruct, Params: 7.5B, Quantization Level: 16bits\n",
      "ModelLayerParams(vocab_size=128256, hidden_size=3072, intermediate_size=8192, num_attention_heads=24, head_dim=128, num_key_value_heads=8, num_hidden_layers=28)\n",
      "Model: meta-llama/Llama-3.2-3B-Instruct, Params: 3.21B, Quantization Level: 16bits\n",
      "ModelLayerParams(vocab_size=128256, hidden_size=8192, intermediate_size=28672, num_attention_heads=64, head_dim=None, num_key_value_heads=8, num_hidden_layers=100)\n",
      "Model: meta-llama/Llama-3.2-90B-Vision-Instruct, Params: 0.0B, Quantization Level: 16bits\n",
      "ModelLayerParams(vocab_size=128256, hidden_size=8192, intermediate_size=28672, num_attention_heads=64, head_dim=128, num_key_value_heads=8, num_hidden_layers=80)\n",
      "Model: meta-llama/Llama-3.3-70B-Instruct, Params: 69.5B, Quantization Level: 16bits\n",
      "ModelLayerParams(vocab_size=256000, hidden_size=3584, intermediate_size=14336, num_attention_heads=16, head_dim=256, num_key_value_heads=8, num_hidden_layers=42)\n",
      "Model: google/gemma-2-9b-it, Params: 9.09B, Quantization Level: 16bits\n",
      "ModelLayerParams(vocab_size=151936, hidden_size=2048, intermediate_size=11008, num_attention_heads=16, head_dim=128, num_key_value_heads=2, num_hidden_layers=36)\n",
      "Model: unsloth/Qwen2.5-3B-unsloth-bnb-4bit, Params: 3.09B, Quantization Level: 4bits\n",
      "ModelLayerParams(vocab_size=129280, hidden_size=7168, intermediate_size=18432, num_attention_heads=128, head_dim=56, num_key_value_heads=128, num_hidden_layers=61)\n",
      "Model: deepseek-ai/DeepSeek-R1, Params: 37.64B, Quantization Level: 8bits\n",
      "ModelLayerParams(vocab_size=151936, hidden_size=2048, intermediate_size=11008, num_attention_heads=16, head_dim=128, num_key_value_heads=2, num_hidden_layers=36)\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, Params: 3.09B, Quantization Level: 16bits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model_ids = [\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-90B-Vision-Instruct\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    \"unsloth/Qwen2.5-3B-unsloth-bnb-4bit\",\n",
    "    \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "]\n",
    "\n",
    "for model_id in model_ids:\n",
    "    model_config = load_model_config(\n",
    "        model_id, \n",
    "        token=os.getenv(\"HF_TOKEN\", None),\n",
    "        # trust_remote_code=True\n",
    "    )\n",
    "    # print(model_config)\n",
    "    model_params = calc_model_params(model_config)\n",
    "    model_quantization_level = calc_model_quantization_level(model_config)\n",
    "    print(f\"Model: {model_id}, Params: {model_params}B, Quantization Level: {model_quantization_level}bits\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-transformers (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
