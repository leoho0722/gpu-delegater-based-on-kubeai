{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prometheus Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import httpx\n",
    "\n",
    "\n",
    "class PrometheusClient:\n",
    "    \"\"\"Prometheus client to interact with the Prometheus server.\"\"\"\n",
    "\n",
    "    _url: str = None\n",
    "    _timeout: float = None\n",
    "    _aclient: httpx.AsyncClient = None\n",
    "\n",
    "    def __init__(self, url: str, timeout: float = 60.0):\n",
    "        \"\"\"\n",
    "        Prometheus client to interact with the Prometheus server.\n",
    "\n",
    "        Args:\n",
    "            url (`str`): URL of the Prometheus server.\n",
    "            timeout (`float`): Timeout for the HTTP requests.\n",
    "        \"\"\"\n",
    "\n",
    "        self.url = url\n",
    "        self.timeout = timeout\n",
    "\n",
    "        self._htimeout = httpx.Timeout(self._timeout)\n",
    "        self._aclient = httpx.AsyncClient(timeout=self._htimeout)\n",
    "\n",
    "    @property\n",
    "    def url(self):\n",
    "        \"\"\"URL of the Prometheus server.\"\"\"\n",
    "\n",
    "        return self._url\n",
    "\n",
    "    @url.setter\n",
    "    def url(self, value):\n",
    "        self._url = value\n",
    "\n",
    "    @property\n",
    "    def timeout(self):\n",
    "        \"\"\"Timeout for the HTTP requests.\"\"\"\n",
    "\n",
    "        return self._timeout\n",
    "\n",
    "    @timeout.setter\n",
    "    def timeout(self, value):\n",
    "        self._timeout = value\n",
    "        self._htimeout = httpx.Timeout(value)\n",
    "        self._aclient = httpx.AsyncClient(timeout=self._htimeout)\n",
    "\n",
    "    async def get_targets(self):\n",
    "        \"\"\"Get all of the targets that Prometheus is scraping.\n",
    "\n",
    "        Returns:\n",
    "            targets (`Dict`): Targets that Prometheus is scraping.\n",
    "        \"\"\"\n",
    "\n",
    "        response = await self._aclient.get(f\"{self.url}/api/v1/targets\")\n",
    "        response.raise_for_status()\n",
    "\n",
    "        return response.json()\n",
    "\n",
    "    async def execute_query(self, query):\n",
    "        \"\"\"Execute a query on the Prometheus server.\n",
    "\n",
    "        Args:\n",
    "            query (`str`): PromQL query to execute\n",
    "\n",
    "        Returns:\n",
    "            response (`Dict`): Query result from the Prometheus server\n",
    "        \"\"\"\n",
    "\n",
    "        response = await self._aclient.get(f\"{self.url}/api/v1/query\", params={\"query\": query})\n",
    "        response.raise_for_status()\n",
    "\n",
    "        return response.json()\n",
    "\n",
    "    async def execute_multiple_queries(self, queries: List[str]) -> Dict[str, Dict]:\n",
    "        \"\"\"Execute multiple queries on the Prometheus server.\n",
    "\n",
    "        Args:\n",
    "            queries (`List[str]`): List of PromQL queries to execute\n",
    "\n",
    "        Returns:\n",
    "            queries_response (`Dict[str, Dict]`): Dictionary of query results from the Prometheus server\n",
    "        \"\"\"\n",
    "\n",
    "        queries_response: Dict[str, Dict] = {}\n",
    "\n",
    "        futures = await asyncio.gather(*[self.execute_query(query) for query in queries])\n",
    "\n",
    "        for i, query in enumerate(queries):\n",
    "            queries_response[query] = futures[i]\n",
    "\n",
    "        return queries_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheme = \"http://\"\n",
    "host = \"10.20.1.93\"\n",
    "port = 30090\n",
    "\n",
    "prometheus_client = PrometheusClient(f\"{scheme}{host}:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Prometheus Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "targets = await prometheus_client.get_targets()\n",
    "# with open(\"targets.json\", \"w\") as f:\n",
    "#     f.write(json.dumps(targets, indent=4))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Multiple Prometheus Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_gpu_info = await prometheus_client.execute_multiple_queries([\n",
    "    \"DCGM_FI_DEV_FB_FREE\",\n",
    "    \"DCGM_FI_DEV_FB_USED\",\n",
    "    \"DCGM_FI_DEV_GPU_TEMP\",\n",
    "    \"DCGM_FI_DEV_GPU_UTIL\",\n",
    "    \"DCGM_FI_DEV_POWER_USAGE\"\n",
    "])\n",
    "\n",
    "# with open(\"dcgm_gpu_info.json\", \"w\") as f:\n",
    "#     f.write(json.dumps(node_gpu_info, indent=4))\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OllamaClient\n\u001b[1;32m      3\u001b[0m ollama_client \u001b[38;5;241m=\u001b[39m OllamaClient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://10.244.0.84:8000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m ollama_client\u001b[38;5;241m.\u001b[39mlist()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llm'"
     ]
    }
   ],
   "source": [
    "from llm.ollama import OllamaClient\n",
    "\n",
    "ollama_client = OllamaClient(\"http://10.244.0.84:8000\")\n",
    "models = await ollama_client.list()\n",
    "print(models.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "from const.format import iB\n",
    "from gpu.dispatcher.types import GPU, GPUNode, GPUNodeList\n",
    "from llm.ollama import ListResponse\n",
    "\n",
    "\n",
    "class GPUDispatcher:\n",
    "\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        '''\n",
    "        GPUDispatcher Singleton Instance\n",
    "        '''\n",
    "\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "\n",
    "        return cls._instance\n",
    "\n",
    "    _node_gpu_info: Dict[str, Dict] = {}\n",
    "    \"\"\"Node GPU Information from Prometheus\"\"\"\n",
    "\n",
    "    _formatted_nodes_gpu_info: GPUNodeList = GPUNodeList([])\n",
    "    \"\"\"Formatted Node GPU Information from Prometheus\"\"\"\n",
    "\n",
    "    def __init__(self, node_gpu_info: Dict[str, Dict]):\n",
    "        '''Initializes the GPUDispatcher to dispatch the GPU resources.\n",
    "\n",
    "        Args:\n",
    "            node_gpu_info (`Dict[str, Dict]`): Node GPU Informaton from Prometheus\n",
    "        '''\n",
    "\n",
    "        self.node_gpu_info = node_gpu_info\n",
    "\n",
    "    @property\n",
    "    def node_gpu_info(self):\n",
    "        \"\"\"Node GPU Information from Prometheus\n",
    "        \"\"\"\n",
    "\n",
    "        return self._node_gpu_info\n",
    "\n",
    "    @node_gpu_info.setter\n",
    "    def node_gpu_info(self, value: Dict[str, Dict]):\n",
    "        self._node_gpu_info = value\n",
    "\n",
    "    @property\n",
    "    def formatted_nodes_gpu_info(self):\n",
    "        \"\"\"Formatted Node GPU Information from Prometheus\n",
    "        \"\"\"\n",
    "\n",
    "        return self._formatted_nodes_gpu_info\n",
    "\n",
    "    @formatted_nodes_gpu_info.setter\n",
    "    def formatted_nodes_gpu_info(self, value: GPUNodeList):\n",
    "        self._formatted_nodes_gpu_info = value\n",
    "\n",
    "    def prometheus_metrics_name_mapping(self, metrics_name: str) -> str:\n",
    "        \"\"\"Mapping Prometheus metrics name to the GPU metrics name.\n",
    "\n",
    "        Args:\n",
    "            metrics_name (`str`): Prometheus metrics name\n",
    "\n",
    "        Returns:\n",
    "            gpu_metrics_name (`str`): GPU metrics name\n",
    "        \"\"\"\n",
    "\n",
    "        match metrics_name:\n",
    "            case \"DCGM_FI_DEV_FB_FREE\": return \"free_memory\"\n",
    "            case \"DCGM_FI_DEV_FB_USED\": return \"used_memory\"\n",
    "            case \"DCGM_FI_DEV_GPU_TEMP\": return \"temperature\"\n",
    "            case \"DCGM_FI_DEV_GPU_UTIL\": return \"memory_usage\"\n",
    "            case \"DCGM_FI_DEV_POWER_USAGE\": return \"power_usage\"\n",
    "\n",
    "    def get_formatted_nodes_gpu_info(self) -> GPUNodeList:\n",
    "        \"\"\"Get the formatted node GPU information from Prometheus.\n",
    "\n",
    "        Returns:\n",
    "            formatted_nodes_gpu_info (`GPUNodeList`): Formatted node GPU information from Prometheus\n",
    "        \"\"\"\n",
    "\n",
    "        gpu_node_list: GPUNodeList = GPUNodeList([])\n",
    "\n",
    "        for query, response in self.node_gpu_info.items():\n",
    "            for node in response[\"data\"][\"result\"]:\n",
    "                node_name = node[\"metric\"][\"kubernetes_node\"]\n",
    "                value = node[\"value\"][1]\n",
    "\n",
    "                existing_node = next(\n",
    "                    (node for node in gpu_node_list.gpu_nodes if node.node_name == node_name), None)\n",
    "                if existing_node:\n",
    "                    gpu_node = existing_node\n",
    "                else:\n",
    "                    gpu_node = GPUNode(node_name, [])\n",
    "                    gpu_node_list.gpu_nodes.append(gpu_node)\n",
    "\n",
    "                gpu_index = node[\"metric\"][\"gpu\"]\n",
    "                gpu_uuid = node[\"metric\"][\"UUID\"]\n",
    "                gpu_name = node[\"metric\"][\"modelName\"]\n",
    "\n",
    "                gpu_info = next(\n",
    "                    (\n",
    "                        gpu for gpu in gpu_node.gpus if gpu.index == f\"cuda:{gpu_index}\"\n",
    "                    ), None\n",
    "                )\n",
    "\n",
    "                if not gpu_info:\n",
    "                    gpu_info = GPU(\n",
    "                        index=f\"cuda:{gpu_index}\",\n",
    "                        uuid=gpu_uuid,\n",
    "                        name=gpu_name,\n",
    "                        free_memory=None,\n",
    "                        used_memory=None,\n",
    "                        temperature=None,\n",
    "                        memory_usage=None,\n",
    "                        power_usage=None\n",
    "                    )\n",
    "\n",
    "                    gpu_node.gpus.append(gpu_info)\n",
    "\n",
    "                for gpu_info in gpu_node.gpus:\n",
    "                    if gpu_info.index == f\"cuda:{gpu_index}\":\n",
    "                        gpu_info.__setattr__(\n",
    "                            self.prometheus_metrics_name_mapping(query),\n",
    "                            int(float(value))\n",
    "                        )\n",
    "\n",
    "        self._formatted_nodes_gpu_info = gpu_node_list\n",
    "\n",
    "        return gpu_node_list\n",
    "\n",
    "    def get_available_gpus(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        ollama_models: ListResponse,\n",
    "        use_multiple_gpus: bool = False\n",
    "    ) -> GPUNodeList:\n",
    "        \"\"\"Get the available GPUs based on the free memory and the estimated VRAM.\n",
    "\n",
    "        Args:\n",
    "            model_name (`str`): Model name for LLM inference\n",
    "            ollama_models (`ListResponse`): Ollama Models\n",
    "            use_multiple_gpus (`bool`): Use multiple GPUs or not\n",
    "\n",
    "        Returns:\n",
    "            available_gpus (`GPUNodeList]`): Available GPUs\n",
    "        \"\"\"\n",
    "\n",
    "        available_gpus: GPUNodeList = GPUNodeList([])\n",
    "\n",
    "        # 估算 LLM 推理所需的 VRAM 大小\n",
    "        estimate_vram = self._calc_inference_estimate_vram(\n",
    "            model_name, ollama_models\n",
    "        )\n",
    "\n",
    "        for gpu_node in self.formatted_nodes_gpu_info.gpu_nodes:\n",
    "            print(f\"Node: {gpu_node.node_name}, GPU Count: {len(gpu_node.gpus)}\")\n",
    "            if len(gpu_node.gpus) > 1:\n",
    "                if use_multiple_gpus:\n",
    "                    total_vram = sum([gpu.free_memory for gpu in gpu_node.gpus])\n",
    "\n",
    "                    self._filter_available_gpus(\n",
    "                        total_vram, estimate_vram, available_gpus, gpu_node.node_name, gpu_node.gpus\n",
    "                    )\n",
    "                else:\n",
    "                    for gpu in gpu_node.gpus:\n",
    "                        self._filter_available_gpus(\n",
    "                            gpu.free_memory, estimate_vram, available_gpus, gpu_node.node_name, gpu_node.gpus\n",
    "                        )\n",
    "            else:\n",
    "                for gpu in gpu_node.gpus:\n",
    "                    self._filter_available_gpus(\n",
    "                        gpu.free_memory, estimate_vram, available_gpus, gpu_node.node_name, gpu_node.gpus\n",
    "                    )\n",
    "\n",
    "        return available_gpus\n",
    "\n",
    "    def _filter_available_gpus(\n",
    "        self,\n",
    "        free_memory: int,\n",
    "        estimate_vram: int,\n",
    "        available_gpus: GPUNodeList,\n",
    "        node: str,\n",
    "        gpus: List[GPU]\n",
    "    ):\n",
    "        \"\"\"Filter the available GPUs based on the free memory and the estimated VRAM.\n",
    "\n",
    "        Args:\n",
    "            free_memory (`int`): Free memory of the GPU.\n",
    "            estimate_vram (`int`): Estimated VRAM required for the LLM inference.\n",
    "            available_gpus (`GPUNodeList`): List to store the available GPUs.\n",
    "            node (`str`): Node name.\n",
    "            gpus (`List[GPUInfo]`): List of GPUs information.\n",
    "        \"\"\"\n",
    "\n",
    "        if free_memory > estimate_vram:\n",
    "            existing_node = next(\n",
    "                (gpu_node for gpu_node in available_gpus.gpu_nodes if gpu_node.node_name == node), None\n",
    "            )\n",
    "\n",
    "            if not existing_node:\n",
    "                existing_node = GPUNode(node_name=node, gpus=[])\n",
    "                available_gpus.gpu_nodes.append(existing_node)\n",
    "\n",
    "            existing_node.gpus.extend(gpus)\n",
    "\n",
    "    def _calc_inference_estimate_vram(self, model_name: str, ollama_models: ListResponse) -> int:\n",
    "        '''\n",
    "        根據模型的 `參數量` 與 `量化等級` 計算進行 LLM 推理所需的預估 GPU 記憶體\n",
    "\n",
    "        Args:\n",
    "            model_name (`str`): 要使用 Ollama 進行 LLM Inference 的模型名稱\n",
    "            ollama_models (`ListResponse`): Ollama Models\n",
    "\n",
    "        Returns:\n",
    "            estimate_vram (`int`): 預估 GPU 記憶體 (MiB)\n",
    "        '''\n",
    "\n",
    "        def re_parameter_size(value: str) -> float:\n",
    "            '''\n",
    "            對 Parameter Size 進行正規化\n",
    "\n",
    "            Args:\n",
    "                value (`str`): Parameter Size 字串，例如：`\"1.2B\"`、`\"8.0B\"`\n",
    "\n",
    "            Returns:\n",
    "                number (`float`): 正規化後的數值\n",
    "            '''\n",
    "\n",
    "            match = re.match(r\"(\\d+(\\.\\d+)?)([KMB])\", value)\n",
    "            if not match:\n",
    "                raise ValueError(\"Invalid format\")\n",
    "\n",
    "            number = float(match.group(1))\n",
    "\n",
    "            return number\n",
    "\n",
    "        def re_quantization_level(value: str) -> int:\n",
    "            '''\n",
    "            對 Quantization Level 進行正規化\n",
    "\n",
    "            Args:\n",
    "                value (`str`): Quantization Level 字串，例如：`\"Q8_0\"`、`\"Q4_K_M\"`、`\"Q4_0\"`、`\"F16\"`\n",
    "\n",
    "            Returns:\n",
    "                number (`int`): 正規化後的數值\n",
    "            '''\n",
    "\n",
    "            match = re.search(r'\\d+', value)\n",
    "            if not match:\n",
    "                raise ValueError(\"No number found in the string\")\n",
    "\n",
    "            return int(match.group(0))\n",
    "\n",
    "        for model in ollama_models.models:\n",
    "            if model.model == model_name:\n",
    "                model_parameter_size = model.details.parameter_size\n",
    "                model_quantization_level = model.details.quantization_level\n",
    "\n",
    "                parameter_size = re_parameter_size(model_parameter_size)\n",
    "                quantization_level = re_quantization_level(\n",
    "                    model_quantization_level)\n",
    "\n",
    "                # 計算公式參考：https://www.substratus.ai/blog/calculating-gpu-memory-for-llm\n",
    "                # `result = ((parameter_size * 4 / (32 / quantization_level)) * 1.2) * iB`\n",
    "                # `result` 為估計的 VRAM 使用量 (MiB)\n",
    "                # `parameter_size` 為模型參數量 (B)\n",
    "                # `quantization_level` 為模型量化等級\n",
    "                # `iB` 為 1024，用來將 GiB 轉換成 MiB\n",
    "                # `1.2` 多計算 20% 的 GPU 記憶體，避免記憶體不足\n",
    "\n",
    "                estimate_vram = math.ceil(\n",
    "                    ((parameter_size * 4 / (32 / quantization_level)) * 1.2) * iB\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"Model: {model_name}, Estimate VRAM: {estimate_vram} MiB\"\n",
    "                )\n",
    "\n",
    "                return estimate_vram\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DCGM_FI_DEV_FB_FREE': {'status': 'success', 'data': {'resultType': 'vector', 'result': [{'metric': {'DCGM_FI_DRIVER_VERSION': '550.54.15', 'Hostname': 'ubuntu-d830mt', 'UUID': 'GPU-956ed35b-d92b-df08-0aa1-9a542e58350b', '__name__': 'DCGM_FI_DEV_FB_FREE', 'device': 'nvidia0', 'gpu': '0', 'instance': '10.244.0.80:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-d830mt', 'modelName': 'NVIDIA GeForce RTX 3070 Ti', 'pci_bus_id': '00000000:01:00.0'}, 'value': [1737101573.721, '7955']}, {'metric': {'DCGM_FI_DRIVER_VERSION': '535.129.03', 'Hostname': 'ubuntu-ms-7d98', 'UUID': 'GPU-67b02881-4c95-3576-6996-9a3f6c30abdc', '__name__': 'DCGM_FI_DEV_FB_FREE', 'device': 'nvidia0', 'gpu': '0', 'instance': '10.244.1.14:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-ms-7d98', 'modelName': 'NVIDIA GeForce RTX 4070', 'pci_bus_id': '00000000:01:00.0'}, 'value': [1737101573.721, '11996']}, {'metric': {'DCGM_FI_DRIVER_VERSION': '535.129.03', 'Hostname': 'ubuntu-ms-7d98', 'UUID': 'GPU-90416dcd-a5d6-8f79-48d9-1ead45f5c327', '__name__': 'DCGM_FI_DEV_FB_FREE', 'device': 'nvidia1', 'gpu': '1', 'instance': '10.244.1.14:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-ms-7d98', 'modelName': 'NVIDIA GeForce RTX 4070', 'pci_bus_id': '00000000:03:00.0'}, 'value': [1737101573.721, '11996']}]}}, 'DCGM_FI_DEV_FB_USED': {'status': 'success', 'data': {'resultType': 'vector', 'result': [{'metric': {'DCGM_FI_DRIVER_VERSION': '550.54.15', 'Hostname': 'ubuntu-d830mt', 'UUID': 'GPU-956ed35b-d92b-df08-0aa1-9a542e58350b', '__name__': 'DCGM_FI_DEV_FB_USED', 'device': 'nvidia0', 'gpu': '0', 'instance': '10.244.0.80:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-d830mt', 'modelName': 'NVIDIA GeForce RTX 3070 Ti', 'pci_bus_id': '00000000:01:00.0'}, 'value': [1737101573.724, '11']}, {'metric': {'DCGM_FI_DRIVER_VERSION': '535.129.03', 'Hostname': 'ubuntu-ms-7d98', 'UUID': 'GPU-67b02881-4c95-3576-6996-9a3f6c30abdc', '__name__': 'DCGM_FI_DEV_FB_USED', 'device': 'nvidia0', 'gpu': '0', 'instance': '10.244.1.14:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-ms-7d98', 'modelName': 'NVIDIA GeForce RTX 4070', 'pci_bus_id': '00000000:01:00.0'}, 'value': [1737101573.724, '12']}, {'metric': {'DCGM_FI_DRIVER_VERSION': '535.129.03', 'Hostname': 'ubuntu-ms-7d98', 'UUID': 'GPU-90416dcd-a5d6-8f79-48d9-1ead45f5c327', '__name__': 'DCGM_FI_DEV_FB_USED', 'device': 'nvidia1', 'gpu': '1', 'instance': '10.244.1.14:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-ms-7d98', 'modelName': 'NVIDIA GeForce RTX 4070', 'pci_bus_id': '00000000:03:00.0'}, 'value': [1737101573.724, '12']}]}}, 'DCGM_FI_DEV_GPU_TEMP': {'status': 'success', 'data': {'resultType': 'vector', 'result': [{'metric': {'DCGM_FI_DRIVER_VERSION': '550.54.15', 'Hostname': 'ubuntu-d830mt', 'UUID': 'GPU-956ed35b-d92b-df08-0aa1-9a542e58350b', '__name__': 'DCGM_FI_DEV_GPU_TEMP', 'device': 'nvidia0', 'gpu': '0', 'instance': '10.244.0.80:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-d830mt', 'modelName': 'NVIDIA GeForce RTX 3070 Ti', 'pci_bus_id': '00000000:01:00.0'}, 'value': [1737101573.725, '48']}, {'metric': {'DCGM_FI_DRIVER_VERSION': '535.129.03', 'Hostname': 'ubuntu-ms-7d98', 'UUID': 'GPU-67b02881-4c95-3576-6996-9a3f6c30abdc', '__name__': 'DCGM_FI_DEV_GPU_TEMP', 'device': 'nvidia0', 'gpu': '0', 'instance': '10.244.1.14:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-ms-7d98', 'modelName': 'NVIDIA GeForce RTX 4070', 'pci_bus_id': '00000000:01:00.0'}, 'value': [1737101573.725, '40']}, {'metric': {'DCGM_FI_DRIVER_VERSION': '535.129.03', 'Hostname': 'ubuntu-ms-7d98', 'UUID': 'GPU-90416dcd-a5d6-8f79-48d9-1ead45f5c327', '__name__': 'DCGM_FI_DEV_GPU_TEMP', 'device': 'nvidia1', 'gpu': '1', 'instance': '10.244.1.14:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-ms-7d98', 'modelName': 'NVIDIA GeForce RTX 4070', 'pci_bus_id': '00000000:03:00.0'}, 'value': [1737101573.725, '39']}]}}, 'DCGM_FI_DEV_GPU_UTIL': {'status': 'success', 'data': {'resultType': 'vector', 'result': [{'metric': {'DCGM_FI_DRIVER_VERSION': '550.54.15', 'Hostname': 'ubuntu-d830mt', 'UUID': 'GPU-956ed35b-d92b-df08-0aa1-9a542e58350b', '__name__': 'DCGM_FI_DEV_GPU_UTIL', 'device': 'nvidia0', 'gpu': '0', 'instance': '10.244.0.80:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-d830mt', 'modelName': 'NVIDIA GeForce RTX 3070 Ti', 'pci_bus_id': '00000000:01:00.0'}, 'value': [1737101573.725, '0']}, {'metric': {'DCGM_FI_DRIVER_VERSION': '535.129.03', 'Hostname': 'ubuntu-ms-7d98', 'UUID': 'GPU-67b02881-4c95-3576-6996-9a3f6c30abdc', '__name__': 'DCGM_FI_DEV_GPU_UTIL', 'device': 'nvidia0', 'gpu': '0', 'instance': '10.244.1.14:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-ms-7d98', 'modelName': 'NVIDIA GeForce RTX 4070', 'pci_bus_id': '00000000:01:00.0'}, 'value': [1737101573.725, '0']}, {'metric': {'DCGM_FI_DRIVER_VERSION': '535.129.03', 'Hostname': 'ubuntu-ms-7d98', 'UUID': 'GPU-90416dcd-a5d6-8f79-48d9-1ead45f5c327', '__name__': 'DCGM_FI_DEV_GPU_UTIL', 'device': 'nvidia1', 'gpu': '1', 'instance': '10.244.1.14:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-ms-7d98', 'modelName': 'NVIDIA GeForce RTX 4070', 'pci_bus_id': '00000000:03:00.0'}, 'value': [1737101573.725, '0']}]}}, 'DCGM_FI_DEV_POWER_USAGE': {'status': 'success', 'data': {'resultType': 'vector', 'result': [{'metric': {'DCGM_FI_DRIVER_VERSION': '550.54.15', 'Hostname': 'ubuntu-d830mt', 'UUID': 'GPU-956ed35b-d92b-df08-0aa1-9a542e58350b', '__name__': 'DCGM_FI_DEV_POWER_USAGE', 'device': 'nvidia0', 'gpu': '0', 'instance': '10.244.0.80:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-d830mt', 'modelName': 'NVIDIA GeForce RTX 3070 Ti', 'pci_bus_id': '00000000:01:00.0'}, 'value': [1737101573.725, '17.054']}, {'metric': {'DCGM_FI_DRIVER_VERSION': '535.129.03', 'Hostname': 'ubuntu-ms-7d98', 'UUID': 'GPU-67b02881-4c95-3576-6996-9a3f6c30abdc', '__name__': 'DCGM_FI_DEV_POWER_USAGE', 'device': 'nvidia0', 'gpu': '0', 'instance': '10.244.1.14:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-ms-7d98', 'modelName': 'NVIDIA GeForce RTX 4070', 'pci_bus_id': '00000000:01:00.0'}, 'value': [1737101573.725, '5.346']}, {'metric': {'DCGM_FI_DRIVER_VERSION': '535.129.03', 'Hostname': 'ubuntu-ms-7d98', 'UUID': 'GPU-90416dcd-a5d6-8f79-48d9-1ead45f5c327', '__name__': 'DCGM_FI_DEV_POWER_USAGE', 'device': 'nvidia1', 'gpu': '1', 'instance': '10.244.1.14:9400', 'job': 'gpu-metrics', 'kubernetes_node': 'ubuntu-ms-7d98', 'modelName': 'NVIDIA GeForce RTX 4070', 'pci_bus_id': '00000000:03:00.0'}, 'value': [1737101573.725, '3.921']}]}}}\n",
      "--------------------------------------------------------------------------------\n",
      "Model: gemma2:2b, Estimate VRAM: 1598 MiB\n",
      "Node: ubuntu-d830mt, GPU Count: 1\n",
      "Node: ubuntu-ms-7d98, GPU Count: 2\n",
      "GPUNodeList(gpu_nodes=[GPUNode(node_name='ubuntu-d830mt', gpu_infos=[GPUInfo(index='cuda:0', uuid='GPU-956ed35b-d92b-df08-0aa1-9a542e58350b', name='NVIDIA GeForce RTX 3070 Ti', free_memory=7955, used_memory=11, memory_usage=0, temperature=48, power_usage=17)]), GPUNode(node_name='ubuntu-ms-7d98', gpu_infos=[GPUInfo(index='cuda:0', uuid='GPU-67b02881-4c95-3576-6996-9a3f6c30abdc', name='NVIDIA GeForce RTX 4070', free_memory=11996, used_memory=12, memory_usage=0, temperature=40, power_usage=5), GPUInfo(index='cuda:1', uuid='GPU-90416dcd-a5d6-8f79-48d9-1ead45f5c327', name='NVIDIA GeForce RTX 4070', free_memory=11996, used_memory=12, memory_usage=0, temperature=39, power_usage=3)])])\n"
     ]
    }
   ],
   "source": [
    "gpu_dispatcher = GPUDispatcher(node_gpu_info)\n",
    "nodes_gpu_info = gpu_dispatcher.get_formatted_nodes_gpu_info()\n",
    "print(node_gpu_info)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "model_name = \"gemma2:2b\"\n",
    "available_gpus = gpu_dispatcher.get_available_gpus(\n",
    "    model_name=model_name,\n",
    "    ollama_models=models,\n",
    "    use_multiple_gpus=True\n",
    ")\n",
    "print(available_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUNodeList(gpu_nodes=[GPUNode(node_name='ubuntu-d830mt', gpu_infos=[GPUInfo(index='cuda:0', uuid='GPU-956ed35b-d92b-df08-0aa1-9a542e58350b', name='NVIDIA GeForce RTX 3070 Ti', free_memory=7955, used_memory=11, memory_usage=0, temperature=48, power_usage=17)]), GPUNode(node_name='ubuntu-ms-7d98', gpu_infos=[GPUInfo(index='cuda:0', uuid='GPU-67b02881-4c95-3576-6996-9a3f6c30abdc', name='NVIDIA GeForce RTX 4070', free_memory=11996, used_memory=12, memory_usage=0, temperature=40, power_usage=5), GPUInfo(index='cuda:1', uuid='GPU-90416dcd-a5d6-8f79-48d9-1ead45f5c327', name='NVIDIA GeForce RTX 4070', free_memory=11996, used_memory=12, memory_usage=0, temperature=39, power_usage=3)])])\n"
     ]
    }
   ],
   "source": [
    "from gpu.dispatcher.types import GPUNodeList, GPUNode, GPU\n",
    "\n",
    "gpu_node_list: GPUNodeList = GPUNodeList([])\n",
    "\n",
    "for query, response in gpu_dispatcher.node_gpu_info.items():\n",
    "    for node in response[\"data\"][\"result\"]:\n",
    "        node_name = node[\"metric\"][\"kubernetes_node\"]\n",
    "        value = node[\"value\"][1]\n",
    "\n",
    "        existing_node = next(\n",
    "            (node for node in gpu_node_list.gpu_nodes if node.node_name == node_name), None)\n",
    "        if existing_node:\n",
    "            gpu_node = existing_node\n",
    "        else:\n",
    "            gpu_node = GPUNode(node_name, [])\n",
    "            gpu_node_list.gpu_nodes.append(gpu_node)\n",
    "\n",
    "        gpu_index = node[\"metric\"][\"gpu\"]\n",
    "        gpu_uuid = node[\"metric\"][\"UUID\"]\n",
    "        gpu_name = node[\"metric\"][\"modelName\"]\n",
    "\n",
    "        gpu_info = next(\n",
    "            (\n",
    "                gpu for gpu in gpu_node.gpus if gpu.index == f\"cuda:{gpu_index}\"\n",
    "            ), None\n",
    "        )\n",
    "\n",
    "        if not gpu_info:\n",
    "            gpu_info = GPU(\n",
    "                index=f\"cuda:{gpu_index}\",\n",
    "                uuid=gpu_uuid,\n",
    "                name=gpu_name,\n",
    "                free_memory=None,\n",
    "                used_memory=None,\n",
    "                temperature=None,\n",
    "                memory_usage=None,\n",
    "                power_usage=None\n",
    "            )\n",
    "\n",
    "            gpu_node.gpus.append(gpu_info)\n",
    "\n",
    "        for gpu_info in gpu_node.gpus:\n",
    "            if gpu_info.index == f\"cuda:{gpu_index}\":\n",
    "                gpu_info.__setattr__(\n",
    "                    gpu_dispatcher.metrics_name_mapping(query), int(float(value)))\n",
    "\n",
    "print(gpu_node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import AsyncClient, ListResponse\n",
    "\n",
    "\n",
    "class OllamaClient:\n",
    "\n",
    "    _aclient: AsyncClient = None\n",
    "\n",
    "    def __init__(self, host: str):\n",
    "        self._aclient = AsyncClient(host)\n",
    "\n",
    "    async def list(self) -> ListResponse:\n",
    "        \"\"\"List Ollama Models\n",
    "\n",
    "        Returns:\n",
    "            models (`ListResponse`): List of Ollama Models\n",
    "        \"\"\"\n",
    "\n",
    "        return await self._aclient.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"models\": [\n",
      "        {\n",
      "            \"model\": \"gemma2:2b\",\n",
      "            \"modified_at\": \"2025-01-02T15:39:30.242118Z\",\n",
      "            \"digest\": \"8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7\",\n",
      "            \"size\": 1629518495,\n",
      "            \"details\": {\n",
      "                \"parent_model\": \"\",\n",
      "                \"format\": \"gguf\",\n",
      "                \"family\": \"gemma2\",\n",
      "                \"families\": [\n",
      "                    \"gemma2\"\n",
      "                ],\n",
      "                \"parameter_size\": \"2.6B\",\n",
      "                \"quantization_level\": \"Q4_0\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"model\": \"gemma2-2b-builtin:latest\",\n",
      "            \"modified_at\": \"2025-01-02T15:39:30.278117Z\",\n",
      "            \"digest\": \"8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7\",\n",
      "            \"size\": 1629518495,\n",
      "            \"details\": {\n",
      "                \"parent_model\": \"\",\n",
      "                \"format\": \"gguf\",\n",
      "                \"family\": \"gemma2\",\n",
      "                \"families\": [\n",
      "                    \"gemma2\"\n",
      "                ],\n",
      "                \"parameter_size\": \"2.6B\",\n",
      "                \"quantization_level\": \"Q4_0\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "\n",
    "def _calc_inference_estimate_vram(model_name: str, ollama_models: ListResponse) -> int:\n",
    "    '''\n",
    "    根據模型的 `參數量` 與 `量化等級` 計算進行 LLM 推理所需的預估 GPU 記憶體\n",
    "\n",
    "    Args:\n",
    "        model_name (`str`): 要使用 Ollama 進行 LLM Inference 的模型名稱\n",
    "        ollama_models (`ListResponse`): Ollama Models\n",
    "\n",
    "    Returns:\n",
    "        estimate_vram (`int`): 預估 GPU 記憶體 (MiB)\n",
    "    '''\n",
    "\n",
    "    def re_parameter_size(value: str) -> float:\n",
    "        '''\n",
    "        對 Parameter Size 進行正規化\n",
    "\n",
    "        Args:\n",
    "            value (`str`): Parameter Size 字串，例如：`\"1.2B\"`、`\"8.0B\"`\n",
    "\n",
    "        Returns:\n",
    "            number (`float`): 正規化後的數值\n",
    "        '''\n",
    "\n",
    "        match = re.match(r\"(\\d+(\\.\\d+)?)([KMB])\", value)\n",
    "        if not match:\n",
    "            raise ValueError(\"Invalid format\")\n",
    "\n",
    "        number = float(match.group(1))\n",
    "\n",
    "        return number\n",
    "\n",
    "    def re_quantization_level(value: str) -> int:\n",
    "        '''\n",
    "        對 Quantization Level 進行正規化\n",
    "\n",
    "        Args:\n",
    "            value (`str`): Quantization Level 字串，例如：`\"Q8_0\"`、`\"Q4_K_M\"`、`\"Q4_0\"`、`\"F16\"`\n",
    "\n",
    "        Returns:\n",
    "            number (`int`): 正規化後的數值\n",
    "        '''\n",
    "\n",
    "        match = re.search(r'\\d+', value)\n",
    "        if not match:\n",
    "            raise ValueError(\"No number found in the string\")\n",
    "\n",
    "        return int(match.group(0))\n",
    "\n",
    "    for model in ollama_models.models:\n",
    "        if model.model == model_name:\n",
    "            model_parameter_size = model.details.parameter_size\n",
    "            model_quantization_level = model.details.quantization_level\n",
    "\n",
    "            parameter_size = re_parameter_size(model_parameter_size)\n",
    "            quantization_level = re_quantization_level(\n",
    "                model_quantization_level)\n",
    "\n",
    "            # 計算公式參考：https://www.substratus.ai/blog/calculating-gpu-memory-for-llm\n",
    "            # `result = ((parameter_size * 4 / (32 / quantization_level)) * 1.2) * iB`\n",
    "            # `result` 為估計的 VRAM 使用量 (MiB)\n",
    "            # `parameter_size` 為模型參數量 (B)\n",
    "            # `quantization_level` 為模型量化等級\n",
    "            # `iB` 為 1024，用來將 GiB 轉換成 MiB\n",
    "            # `1.2` 多計算 20% 的 GPU 記憶體，避免記憶體不足\n",
    "\n",
    "            estimate_vram = math.ceil(\n",
    "                ((parameter_size * 4 / (32 / quantization_level)) * 1.2) * 1024\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Model: {model_name}, Estimate VRAM: {estimate_vram} MiB\"\n",
    "            )\n",
    "\n",
    "            return estimate_vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gemma2:2b, Estimate VRAM: 1598 MiB\n",
      "1598\n"
     ]
    }
   ],
   "source": [
    "estimated_vram = _calc_inference_estimate_vram(\"gemma2:2b\", models)\n",
    "print(estimated_vram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轉換後的模型名稱：\n",
      "gemma2-2b → gemma2:2b\n",
      "gemma2-9b → gemma2:9b\n",
      "llama3.1-8b → llama3.1:8b\n",
      "llama3.2-3b → llama3.2:3b\n",
      "llama3.3-70b-instruct-fp16 → llama3.3:70b-instruct-fp16\n",
      "llama3.2:1b → llama3.2:1b\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def convert_ollama_model_names(model_names: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    將Ollama模型名稱轉換為冒號分隔的格式。\n",
    "    \n",
    "    參數:\n",
    "        model_names (List[str]): 要轉換的模型名稱列表\n",
    "    \n",
    "    返回:\n",
    "        List[str]: 轉換後的模型名稱\n",
    "    \n",
    "    拋出:\n",
    "        ValueError: 如果輸入不是列表\n",
    "    \"\"\"\n",
    "    def convert_single_model_name(name: str) -> str:\n",
    "        # 更精確的正則表達式，支持更複雜的模型名稱模式\n",
    "        pattern = r'^([\\w\\d.]+)(?:[-.](\\d+[a-z]?(?:-\\w+)*))?(?:[:.](\\d+[a-z]?(?:-\\w+)*))?'\n",
    "        match = re.match(pattern, name)\n",
    "        \n",
    "        if match:\n",
    "            model_base = match.group(1)\n",
    "            version_part1 = match.group(2)\n",
    "            version_part2 = match.group(3)\n",
    "            \n",
    "            # 根據不同的匹配情況進行轉換\n",
    "            if version_part1 and not version_part2:\n",
    "                # 原始模式：model-version\n",
    "                return f\"{model_base}:{version_part1}\"\n",
    "            elif version_part2:\n",
    "                # 已包含冒號的模式：model:version\n",
    "                return name\n",
    "            else:\n",
    "                # 無版本的情況\n",
    "                return name\n",
    "        \n",
    "        # 如果無法進行轉換，則返回原始名稱\n",
    "        return name\n",
    "    \n",
    "    # 輸入驗證\n",
    "    if not isinstance(model_names, list):\n",
    "        raise ValueError(\"輸入必須是模型名稱列表\")\n",
    "    \n",
    "    # 轉換每個模型名稱\n",
    "    converted_names = [convert_single_model_name(name) for name in model_names]\n",
    "    \n",
    "    return converted_names\n",
    "\n",
    "# 示例使用和測試\n",
    "def main():\n",
    "    input_models = [\n",
    "        \"gemma2-2b\", \n",
    "        \"gemma2-9b\", \n",
    "        \"llama3.1-8b\", \n",
    "        \"llama3.2-3b\", \n",
    "        \"llama3.3-70b-instruct-fp16\",\n",
    "        \"llama3.2:1b\"  # 新增測試用例\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        output_models = convert_ollama_model_names(input_models)\n",
    "        print(\"轉換後的模型名稱：\")\n",
    "        for orig, converted in zip(input_models, output_models):\n",
    "            print(f\"{orig} → {converted}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"轉換錯誤：{e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OllamaBuiltinModel.Gemma2_2B\n"
     ]
    }
   ],
   "source": [
    "from llm.models import  OllamaBuiltinModel\n",
    "\n",
    "def get_ollama_model(model_name: str) -> OllamaBuiltinModel:\n",
    "    \n",
    "    try:\n",
    "        return OllamaBuiltinModel(model_name)\n",
    "    except ValueError as e:\n",
    "        print(f\"無法獲取模型：{e}\")\n",
    "        \n",
    "model = get_ollama_model(\"gemma2:2b\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"apiVersion\": \"kubeai.org/v1\",\n",
      "    \"kind\": \"Model\",\n",
      "    \"metadata\": {\n",
      "        \"name\": \"gemma2-2b\"\n",
      "    },\n",
      "    \"spec\": {\n",
      "        \"features\": [\n",
      "            \"TextGeneration\"\n",
      "        ],\n",
      "        \"image\": \"leoho0722/ollama-builtin-gemma2-2b:0.1.0\",\n",
      "        \"url\": \"ollama://gemma2:2b\",\n",
      "        \"engine\": \"OLlama\",\n",
      "        \"replicas\": 1,\n",
      "        \"minReplicas\": 1,\n",
      "        \"targetRequests\": 50,\n",
      "        \"scaleDownDelaySeconds\": 30,\n",
      "        \"env\": {\n",
      "            \"OLLAMA_KEEP_ALIVE\": \"0s\"\n",
      "        },\n",
      "        \"resourceProfile\": \"cpu:2\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"apiVersion\": \"kubeai.org/v1\",\n",
      "    \"kind\": \"Model\",\n",
      "    \"metadata\": {\n",
      "        \"name\": \"gemma2-9b\"\n",
      "    },\n",
      "    \"spec\": {\n",
      "        \"features\": [\n",
      "            \"TextGeneration\"\n",
      "        ],\n",
      "        \"image\": \"leoho0722/ollama-builtin-gemma2-9b:0.1.0\",\n",
      "        \"url\": \"ollama://gemma2:9b\",\n",
      "        \"engine\": \"OLlama\",\n",
      "        \"replicas\": 1,\n",
      "        \"minReplicas\": 1,\n",
      "        \"targetRequests\": 50,\n",
      "        \"scaleDownDelaySeconds\": 30,\n",
      "        \"env\": {\n",
      "            \"OLLAMA_KEEP_ALIVE\": \"0s\"\n",
      "        },\n",
      "        \"resourceProfile\": \"cpu:4\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"apiVersion\": \"kubeai.org/v1\",\n",
      "    \"kind\": \"Model\",\n",
      "    \"metadata\": {\n",
      "        \"name\": \"llama3.1-8b\"\n",
      "    },\n",
      "    \"spec\": {\n",
      "        \"features\": [\n",
      "            \"TextGeneration\"\n",
      "        ],\n",
      "        \"image\": \"leoho0722/ollama-builtin-llama3.1-8b:0.1.0\",\n",
      "        \"url\": \"ollama://llama3.1:8b\",\n",
      "        \"engine\": \"OLlama\",\n",
      "        \"replicas\": 1,\n",
      "        \"minReplicas\": 1,\n",
      "        \"targetRequests\": 50,\n",
      "        \"scaleDownDelaySeconds\": 30,\n",
      "        \"env\": {\n",
      "            \"OLLAMA_KEEP_ALIVE\": \"0s\"\n",
      "        },\n",
      "        \"resourceProfile\": \"cpu:4\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"apiVersion\": \"kubeai.org/v1\",\n",
      "    \"kind\": \"Model\",\n",
      "    \"metadata\": {\n",
      "        \"name\": \"llama3.2-3b\"\n",
      "    },\n",
      "    \"spec\": {\n",
      "        \"features\": [\n",
      "            \"TextGeneration\"\n",
      "        ],\n",
      "        \"image\": \"leoho0722/ollama-builtin-llama3.2-3b:0.1.0\",\n",
      "        \"url\": \"ollama://llama3.2:3b\",\n",
      "        \"engine\": \"OLlama\",\n",
      "        \"replicas\": 1,\n",
      "        \"minReplicas\": 1,\n",
      "        \"targetRequests\": 50,\n",
      "        \"scaleDownDelaySeconds\": 30,\n",
      "        \"env\": {\n",
      "            \"OLLAMA_KEEP_ALIVE\": \"0s\"\n",
      "        },\n",
      "        \"resourceProfile\": \"cpu:2\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Any, Dict\n",
    "\n",
    "import yaml\n",
    "\n",
    "def parse_model_yaml(model_yaml_file_path: str) -> Dict[str, Any]:\n",
    "    with open(model_yaml_file_path, 'r') as f:\n",
    "        model_yaml: Dict[str, Any] = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "        return model_yaml\n",
    "    \n",
    "gemma2_2b_yaml = parse_model_yaml(\"k8s/deploy/kubeai/gemma2-2b-builtin.yaml\")\n",
    "print(json.dumps(gemma2_2b_yaml, indent=4))\n",
    "\n",
    "gemma2_9b_yaml = parse_model_yaml(\"k8s/deploy/kubeai/gemma2-9b-builtin.yaml\")\n",
    "print(json.dumps(gemma2_9b_yaml, indent=4))\n",
    "\n",
    "llama3_1_8b_yaml = parse_model_yaml(\"k8s/deploy/kubeai/llama3.1-8b-builtin.yaml\")\n",
    "print(json.dumps(llama3_1_8b_yaml, indent=4))\n",
    "\n",
    "llama3_2_3b_yaml = parse_model_yaml(\"k8s/deploy/kubeai/llama3.2-3b-builtin.yaml\")\n",
    "print(json.dumps(llama3_2_3b_yaml, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_available_gpus(\n",
    "    self,\n",
    "    model_name: str,\n",
    "    ollama_models: ListResponse,\n",
    "    use_multiple_gpus: bool = True  # 預設改為 True 以支援多 GPU\n",
    ") -> GPUNodeList:\n",
    "    available_gpus: GPUNodeList = GPUNodeList()\n",
    "    estimate_vram = _calc_inference_estimate_vram(model_name, ollama_models)\n",
    "    \n",
    "    for gpu_node in self.formatted_nodes_gpu_info.gpu_nodes:\n",
    "        # 依照 VRAM 大小排序 GPU (從小到大)\n",
    "        sorted_gpus = sorted(gpu_node.gpus, key=lambda x: x.free_memory)\n",
    "        total_node_vram = sum(gpu.free_memory for gpu in gpu_node.gpus)\n",
    "        \n",
    "        print(f\"Node: {gpu_node.node_name}, Total VRAM: {total_node_vram}, Required VRAM: {estimate_vram}\")\n",
    "        \n",
    "        # 檢查節點總 VRAM 是否足夠\n",
    "        if total_node_vram >= estimate_vram:\n",
    "            required_gpus = []\n",
    "            current_vram = 0\n",
    "            \n",
    "            # 從 VRAM 較小的 GPU 開始選擇\n",
    "            for gpu in sorted_gpus:\n",
    "                required_gpus.append(gpu)\n",
    "                current_vram += gpu.free_memory\n",
    "                \n",
    "                if current_vram >= estimate_vram:\n",
    "                    # 找到足夠的 GPU 組合\n",
    "                    existing_node = GPUNode(node_name=gpu_node.node_name, gpus=[])\n",
    "                    existing_node.gpus.extend(required_gpus)\n",
    "                    available_gpus.gpu_nodes.append(existing_node)\n",
    "                    break\n",
    "        \n",
    "        print(\n",
    "            f\"Node {gpu_node.node_name}: Selected {len(available_gpus.gpu_nodes)} GPU(s)\"\n",
    "        )\n",
    "\n",
    "    return available_gpus\n",
    "\n",
    "def _calc_inference_estimate_vram(self, model_name: str, ollama_models: ListResponse) -> int:\n",
    "    def re_parameter_size(value: str) -> float:\n",
    "        '''\n",
    "        對 Parameter Size 進行正規化\n",
    "\n",
    "        Args:\n",
    "            value (`str`): Parameter Size 字串，例如：`\"1.2B\"`、`\"8.0B\"`\n",
    "\n",
    "        Returns:\n",
    "            number (`float`): 正規化後的數值\n",
    "        '''\n",
    "\n",
    "        match = re.match(r\"(\\d+(\\.\\d+)?)([KMB])\", value)\n",
    "        if not match:\n",
    "            raise ValueError(\"Invalid format\")\n",
    "\n",
    "        number = float(match.group(1))\n",
    "\n",
    "        return number\n",
    "\n",
    "    def re_quantization_level(value: str) -> int:\n",
    "        '''\n",
    "        對 Quantization Level 進行正規化\n",
    "\n",
    "        Args:\n",
    "            value (`str`): Quantization Level 字串，例如：`\"Q8_0\"`、`\"Q4_K_M\"`、`\"Q4_0\"`、`\"F16\"`\n",
    "\n",
    "        Returns:\n",
    "            number (`int`): 正規化後的數值\n",
    "        '''\n",
    "\n",
    "        match = re.search(r'\\d+', value)\n",
    "        if not match:\n",
    "            raise ValueError(\"No number found in the string\")\n",
    "\n",
    "        return int(match.group(0))\n",
    "        \n",
    "    for model in ollama_models.models:\n",
    "        if model.model == model_name:\n",
    "            parameter_size = re_parameter_size(model.details.parameter_size)\n",
    "            quantization_level = re_quantization_level(model.details.quantization_level)\n",
    "\n",
    "            # 計算基礎的 VRAM 需求\n",
    "            base_vram = math.ceil(\n",
    "                ((parameter_size * 4 / (32 / quantization_level))) * iB\n",
    "            )\n",
    "            \n",
    "            # 加上 20% 的緩衝\n",
    "            estimate_vram = math.ceil(base_vram * 1.2)\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"Model: {model_name}, Base VRAM: {base_vram} MiB, Final Estimate: {estimate_vram} MiB\"\n",
    "            )\n",
    "            \n",
    "            return estimate_vram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected GPU Model: NVIDIA GeForce RTX 3070 Ti, Count: 1, Resource Name: nvidia-gpu-3070ti-8gb:1\n",
      "Selected GPU Model: NVIDIA GeForce RTX 3080 Ti, Count: 2, Resource Name: nvidia-gpu-3080ti-12gb:2\n",
      "Selected GPU Model: NVIDIA GeForce RTX 4070, Count: 2, Resource Name: nvidia-gpu-4070-12gb:2\n",
      "Selected GPU Model: NVIDIA GeForce RTX 4090, Count: 1, Resource Name: nvidia-gpu-4090-24gb:1\n"
     ]
    }
   ],
   "source": [
    "def convert_to_kubeai_gpu_resources_name(model: str, count: int) -> str:\n",
    "    \"\"\"Convert the selected GPU resources to the KubeAI GPU resources name.\n",
    "\n",
    "    Args:\n",
    "        selected_gpu (`GPUNode`): Selected GPU resources\n",
    "\n",
    "    Returns:\n",
    "        kubeai_gpu_resources_name (`str`): KubeAI GPU resources name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        selected_gpu_model = model\n",
    "        selected_gpu_count = count\n",
    "        \n",
    "        # GPU VRAM 映射表\n",
    "        gpu_vram_map = {\n",
    "            \"3070 Ti\": \"8\",\n",
    "            \"3080 Ti\": \"12\",\n",
    "            \"3090\": \"24\",\n",
    "            \"4070\": \"12\",\n",
    "            \"4080\": \"16\",\n",
    "            \"4090\": \"24\",\n",
    "        }\n",
    "        \n",
    "        # 解析 GPU 型號\n",
    "        model_number = None\n",
    "        vram_size = None\n",
    "        \n",
    "        for key in gpu_vram_map.keys():\n",
    "            if key in selected_gpu_model:\n",
    "                model_number = key.replace(\" \", \"\").lower()\n",
    "                vram_size = gpu_vram_map[key]\n",
    "                break\n",
    "        \n",
    "        if not model_number or not vram_size:\n",
    "            raise ValueError(f\"Unsupported GPU model: {selected_gpu_model}\")\n",
    "            \n",
    "        # 格式化輸出\n",
    "        kubeai_gpu_resources_name = f\"nvidia-gpu-{model_number}-{vram_size}gb:{selected_gpu_count}\"\n",
    "        \n",
    "        print(\n",
    "            f\"Selected GPU Model: {selected_gpu_model}, \"\n",
    "            f\"Count: {selected_gpu_count}, \"\n",
    "            f\"Resource Name: {kubeai_gpu_resources_name}\"\n",
    "        )\n",
    "        \n",
    "        return kubeai_gpu_resources_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting GPU resources name: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "testing_inputs = [\n",
    "    {\n",
    "        \"model\": \"NVIDIA GeForce RTX 3070 Ti\", \n",
    "        \"count\": 1\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"NVIDIA GeForce RTX 3080 Ti\",\n",
    "        \"count\": 2\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"NVIDIA GeForce RTX 4070\",\n",
    "        \"count\": 2\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"NVIDIA GeForce RTX 4090\",\n",
    "        \"count\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "for input_data in testing_inputs:\n",
    "    convert_to_kubeai_gpu_resources_name(input_data[\"model\"], input_data[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: NVIDIA Geforce RTX 3070 Ti, VRAM: 8\n",
      "Model Number: 3070 Ti\n",
      "Model: NVIDIA Geforce RTX 3080 Ti, VRAM: 12\n",
      "Model Number: 3080 Ti\n",
      "Model: NVIDIA Geforce RTX 4070, VRAM: 12\n",
      "Model Number: 4070\n",
      "Model: NVIDIA Geforce RTX 4080 SUPER, VRAM: 16\n",
      "Model Number: 4080 SUPER\n",
      "Model: NVIDIA Geforce RTX 4090, VRAM: 24\n",
      "Model Number: 4090\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "\n",
    "with open(\"backend/gpu/dispatcher/gpu_models.yaml\") as f:\n",
    "    gpu_models: List[Dict[str, Any]] = yaml.safe_load(f)\n",
    "    f.close()\n",
    "    \n",
    "    for gpu in gpu_models:\n",
    "        model = gpu[\"model\"]\n",
    "        vram = gpu[\"vram\"]\n",
    "        print(f\"Model: {model}, VRAM: {vram}\")\n",
    "        \n",
    "        model = model.split(\"RTX\")[1].strip()\n",
    "        print(f\"Model Number: {model}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
